#
# Copyright 2021 StreamSets Inc.
#

# HTTP configuration

# The base URL of Transformer, used to create email alert messages.
# If not set http://<hostname>:<http.port> is used.
# <hostname> is either taken from http.bindHost or resolved using
# 'hostname -f' if not configured.
#transformer.base.http.url=http://<hostname>:<port>

# Hostname or IP address that Transformer will bind to.
# Default is 0.0.0.0 that will bind to all interfaces.
#http.bindHost=0.0.0.0

# Maximum number of HTTP servicing threads.
#http.maxThreads=200

# The port Transformer runs as the Transformer HTTP endpoint.
# If different that -1, Transformer will run on this port.
# If 0, Transformer will pick up a random port.
# If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint
# will redirect to the HTTPS endpoint.
http.port=19630

# HTTPS configuration

# The port Transformer runs as the Transformer HTTPS endpoint.
# If different that -1, Transformer will run over SSL on this port.
# If 0, the Transformer will pick up a random port.
https.port=-1

# Enables HTTP/2 support for the Transformer UI/REST API. If you are using any clients
# that do not support ALPN for protocol negotiation, leave this option disabled.
http2.enable=false

# Reverse Proxy / Load Balancer configuration

# TRANSFORMER will handle X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port
# headers issued by a reverse proxy such as HAProxy, ELB, nginx when set to true.
# Set to true when hosting TRANSFORMER behind a reverse proxy / load balancer.
http.enable.forwarded.requests=false

# Java keystore file, in the TRANSFORMER 'etc/' configuration directory
https.keystore.path=keystore.jks

# Password for the keystore file,
# By default, the password is loaded from the 'keystore-password.txt'
# from the TRANSFORMER 'etc/' configuration directory
https.keystore.password=${file("keystore-password.txt")}

# Truststore configs
# By default, if below configs are commented then cacerts from JRE lib directory will be used as truststore

# Java truststore file on Spark driver transformer which stores certificates to trust identity of transformer launcher
# in the TRANSFORMER 'etc/' configuration directory.
#https.truststore.path=truststore.jks

# Password for truststore file
# from the TRANSFORMER 'etc/' configuration directory
#https.truststore.password=${file("truststore-password.txt")}

# HTTP Session Timeout
# Max period of inactivity, after which the HTTP session is invalidated, in seconds.
# Default value is 86400 seconds (24 hours)
# value -1 means no timeout
http.session.max.inactive.interval=86400

# The authentication for the HTTP endpoint of Transformer.
# Valid values are: 'none', 'basic', 'digest', 'form' or 'aster'
#
http.authentication=form

# Authentication Login Module
# Valid values are: 'file' and 'ldap'
# For 'file', the authentication and role information is read from a property file (etc/basic-realm.properties,
#   etc/digest-realm.properties or etc/form-realm.properties based on the 'http.authentication' value).
# For 'ldap', the authentication and role information is read from a LDAP Server
#   and LDAP connection information is read from etc/ldap-login.conf.
http.authentication.login.module=file

# The realm used for authentication
# A file with the realm name and '.properties' extension must exist in the Transformer configuration directory
# If this property is not set, the realm name is '<http.authentication>-realm'
#http.digest.realm=local-realm

# Check the permissions of the realm file should be owner only
http.realm.file.permission.check=true

# LDAP group to Transformer role mapping
# the mapping is specified as the following pattern:
#    <ldap-group>:<transformer-role>(,<transformer-role>)*(;<ldap-group>:<transformer-role>(,<transformer-role>)*)*
# e.g.
#    Administrator:admin;Manager:manager;DevOP:creator;Tester:guest;
http.authentication.ldap.role.mapping=

# LDAP login module name as present in the JAAS config file.
# If no value is specified, the login module name is assumed to be "ldap"
ldap.login.module.name=ldap

# HTTP access control (CORS)
http.access.control.allow.origin=*
http.access.control.allow.headers=origin, content-type, cache-control, pragma, accept, authorization, x-requested-by, x-ss-user-auth-token, x-ss-rest-call
http.access.control.exposed.headers=X-SDC-LOG-PREVIOUS-OFFSET
http.access.control.allow.methods=GET, POST, PUT, DELETE, OPTIONS, HEAD

# Application callback inactivity timeout in milliseconds
# Max period of Transformer Spark application callback request inactivity, before stopping the pipeline with a run error.
# The default value is 30000 milliseconds (30 seconds)
transformer.driver.max.inactive.interval=30000

# A maximum number of times to retry an unsuccessful pipeline start on Databricks Cluster.
transformer.databricks.run.max.retries=2

# Minimal interval in milliseconds between the start of the failed run, and the subsequent retry run on the Databricks cluster.
transformer.databricks.run.retry.interval=10000

# Always resolve Kerberos properties
# This option allows Transformer pipelines to reference the keytab and principal declared in this properties file
# If you have a valid kerberos.client.principal and kerberos.client.keytab set below, and want to make them
# available to pipelines within this Transformer instance, then comment out the following line
#kerberos.client.alwaysResolveProperties=true

# The kerberos principal to use for the Kerberos session.
# It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',
# the hostname will be replaced with the actual complete hostname of Transformer as advertised by the
# unix command 'hostname -f'
kerberos.client.principal=transformer/_HOST@EXAMPLE.COM

# The location of the keytab file for the specified principal. If the path is relative, the keytab file will be
# looked under the Transformer configuration directory
kerberos.client.keytab=transformer.keytab

# Uncomment this line to disallow users to specify a keytab to use with pipelines.
# If this option is set to false, then the --proxy-user option will be set to spark-submit and the
# Transformer process will assume that a ticket cache exists already (i.e. is managed by a wrapper script
# such as k5start) and is therefore available for the spark-submit child process
#kerberos.pipeline.keytab.allowed=false

preview.maxBatchSize=1000
preview.maxBatches=10

# Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint
max.logtail.concurrent.requests=5

# Max number of concurrent WebSocket calls allowed
max.webSockets.concurrent.requests=15

# Pipeline Sharing / ACLs
pipeline.access.control.enabled=false

# Customize header title for TRANSFORMER UI
# You can pass any HTML tags here
# Example:
#   For Text  -  <span class="navbar-brand">New Brand Name</span>
#   For Image -  <img src="assets/add.png">
ui.header.title=<span class="navbar-brand">Transformer</span>

ui.local.help.base.url=/docs
ui.hosted.help.base.url=https://www.streamsets.com/documentation/transformer/3.7.0-SNAPSHOT/userguide/help
# ui.account.registration.url=

ui.refresh.interval.ms=2000
ui.jvmMetrics.refresh.interval.ms=4000

# If true TRANSFORMER UI will use WebSocket to fetch pipeline status/metrics/alerts otherwise UI will poll every few seconds
# to get the Pipeline status/metrics/alerts.
ui.enable.webSocket=true

# Number of changes supported by undo/redo functionality.
# UI archives Pipeline Configuration/Rules in browser memory to support undo/redo functionality.
ui.undo.limit=10

# SMTP configuration to send alert emails
# All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'
mail.transport.protocol=smtp
mail.smtp.host=localhost
mail.smtp.port=25
mail.smtp.auth=false
mail.smtp.starttls.enable=false
mail.smtps.host=localhost
mail.smtps.port=465
mail.smtps.auth=false
# If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,
# ${file("email-password.txt")} will load the value from the 'email-password.txt' file in the config directory (where this file is)
xmail.username=foo
xmail.password=${file("email-password.txt")}
# FROM email address to use for the messages
xmail.from.address=transformer@localhost

#Indicates the location where runtime configuration properties can be found.
#Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with
#'runtime.conf_'.
#A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration
#properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.
runtime.conf.location=embedded

# Java Security properties
#
# Any configuration prefixed with 'java.security.<property>' will be set on the static instance java.security.Security
# as part of TRANSFORMER bootstrap process. This will change JVM configuration and should not be used when embedding and running
# multiple TRANSFORMER instances inside the same JVM.
#
# We're explicitly overriding this to zero as JVM will default to -1 if security manager is active.
java.security.networkaddress.cache.ttl=0

# Stage specific configuration(s)
#
# The following config properties are for particular stages, please refer to their documentation for further details.
#
# Hadoop components
# Uncomment to enforce Hadoop components in TRANSFORMER to always impersonate current user rather then use the impersonation
# configuration option. Current user is a user who either started the pipeline or run preview.
#hadoop.always.impersonate.current.user=true
# Uncomment to enforce impersonated user name to be lower cased.
#hadoop.always.lowercase.user=true

# Indicate when Transformer runs on MapR cluster.
#hadoop.mapr.cluster=false

#Observer related

#The size of the queueName where the pipeline queues up data rule evaluation requests.
#Each request is for a stream and contains sampled records for all rules that apply to that lane.
observer.queue.size=100

#Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is
#once cache per data rule
observer.sampled.records.cache.size=100

#The time to wait before dropping a data rule evaluation request if the observer queueName is full.
observer.queue.offer.max.wait.time.ms=1000


#Maximum number of private classloaders to allow in Transformer.
#Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders
max.stage.private.classloaders=50

# Pipeline com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner pool
# Default value is sufficient to run 22 pipelines. One pipeline requires 5 Threads and pipelines share
# threads using thread pool. Approximate com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner thread pool size = (Number of Running Pipelines) * 2.2.
# Increasing this value will not increase parallelisation of individual pipelines.
com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.thread.pool.size=50

# Uncomment to disable starting all previously running pipelines on TRANSFORMER start up
#com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.boot.pipeline.restart=false

# Support bundles
#
# Uncomment if you need to disable the facility for automatic support bundle upload.
#bundle.upload.enabled=false
#
# Uncomment to automatically generate and upload bundle on various errors. Enable with caution, uploading bundle
# can be time consuming task (depending on size and internet speed) and pipelines can appear "frozen" during
# the upload especially when many pipelines are failing at the same time.
#bundle.upload.on_error=true

#
# Additional Configuration files to include in to the configuration.
# Value of this property is the name of the configuration file separated by commas.
#
config.includes=vault.properties,credential-stores.properties

#
# Pipeline State are cached for faster access.
# Specifies the maximum number of pipeline state entries the cache may contain.
store.pipeline.state.cache.maximum.size=100

# Specifies that each pipeline state entry should be automatically removed from the cache once a fixed duration
# has elapsed after the entry's creation, the most recent replacement of its value, or its last access.
# In minutes
store.pipeline.state.cache.expire.after.access=10

# uncomment to skip the Spark version >= 2.3.0 check performed in the Spark app launcher
#skipSparkVersionCheck=true

# base directory for temporary keytabs (used when keytabs are resolved from credential stores)
#transformer.temp-keytabs.location=/tmp/streamsets-transformer

#
# Copyright 2021 StreamSets Inc.
#

#
# Control Hub Enabled
# If true, HTTP Authentication for Transformer will be configured to use Control Hub SSO Authentication.
#
dpm.enabled=false

#
# Base URL of the Remote Service
# In a real deployment the security service must be encrypted (HTTPS)
#
dpm.base.url=http://localhost:18631

#
# Registration attempts.
# There is an exponential backoff that starts with 2 seconds and maxes out at 16 seconds.
#
dpm.registration.retry.attempts=5

#
# Frequency of validation of user and app authentication tokens.
# As part of this validation all information about the principal is refreshed.
#
dpm.security.validationTokenFrequency.secs=60

#
# Application Token
#
dpm.appAuthToken=@application-token.txt@

#
# Labels for this Transformer to report the Control Hub
#
dpm.remote.control.job.labels=all

#
# Transformer Ping Frequency to Control Hub (in milliseconds)
#
dpm.remote.control.ping.frequency=5000

#
# App to send remote control events
#
dpm.remote.control.events.recipient=jobrunner-app

#
# Apps to send Transformer process metrics (CPU Load and Heap Memory Usage)
#
dpm.remote.control.process.events.recipients=jobrunner-app,timeseries-app

#
# Frequency to send pipeline status events (all remote pipelines and local running pipelines) and
# Transformer process metrics like CPU load and heap memory usage
#
dpm.remote.control.status.events.interval = 60000


dpm.remote.deployment.id=

#
# Indicates if the redirection to Control Hub SSO is done using HTML META refresh.
# This is useful for environment that rewrite redirect headers.
#
http.meta.redirect.to.sso=false


# Uncomment to use 'user' as hadoop proxy user from the full user name 'user@org'. Below setting
# only takes effect when Transformer is control hub enabled and stage impersonation is set to true
#
#dpm.alias.name.enabled=true

#
# Component Type
#
dpm.componentType=transformer#
# Copyright 2021 StreamSets Inc.
#

# HTTP configuration

# The base URL of Transformer, used to create email alert messages.
# If not set http://<hostname>:<http.port> is used.
# <hostname> is either taken from http.bindHost or resolved using
# 'hostname -f' if not configured.
#transformer.base.http.url=http://<hostname>:<port>

# Hostname or IP address that Transformer will bind to.
# Default is 0.0.0.0 that will bind to all interfaces.
#http.bindHost=0.0.0.0

# Maximum number of HTTP servicing threads.
#http.maxThreads=200

# The port Transformer runs as the Transformer HTTP endpoint.
# If different that -1, Transformer will run on this port.
# If 0, Transformer will pick up a random port.
# If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint
# will redirect to the HTTPS endpoint.
http.port=19630

# HTTPS configuration

# The port Transformer runs as the Transformer HTTPS endpoint.
# If different that -1, Transformer will run over SSL on this port.
# If 0, the Transformer will pick up a random port.
https.port=-1

# Enables HTTP/2 support for the Transformer UI/REST API. If you are using any clients
# that do not support ALPN for protocol negotiation, leave this option disabled.
http2.enable=false

# Reverse Proxy / Load Balancer configuration

# TRANSFORMER will handle X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port
# headers issued by a reverse proxy such as HAProxy, ELB, nginx when set to true.
# Set to true when hosting TRANSFORMER behind a reverse proxy / load balancer.
http.enable.forwarded.requests=false

# Java keystore file, in the TRANSFORMER 'etc/' configuration directory
https.keystore.path=keystore.jks

# Password for the keystore file,
# By default, the password is loaded from the 'keystore-password.txt'
# from the TRANSFORMER 'etc/' configuration directory
https.keystore.password=${file("keystore-password.txt")}

# Truststore configs
# By default, if below configs are commented then cacerts from JRE lib directory will be used as truststore

# Java truststore file on Spark driver transformer which stores certificates to trust identity of transformer launcher
# in the TRANSFORMER 'etc/' configuration directory.
#https.truststore.path=truststore.jks

# Password for truststore file
# from the TRANSFORMER 'etc/' configuration directory
#https.truststore.password=${file("truststore-password.txt")}

# HTTP Session Timeout
# Max period of inactivity, after which the HTTP session is invalidated, in seconds.
# Default value is 86400 seconds (24 hours)
# value -1 means no timeout
http.session.max.inactive.interval=86400

# The authentication for the HTTP endpoint of Transformer.
# Valid values are: 'none', 'basic', 'digest', 'form' or 'aster'
#
http.authentication=form

# Authentication Login Module
# Valid values are: 'file' and 'ldap'
# For 'file', the authentication and role information is read from a property file (etc/basic-realm.properties,
#   etc/digest-realm.properties or etc/form-realm.properties based on the 'http.authentication' value).
# For 'ldap', the authentication and role information is read from a LDAP Server
#   and LDAP connection information is read from etc/ldap-login.conf.
http.authentication.login.module=file

# The realm used for authentication
# A file with the realm name and '.properties' extension must exist in the Transformer configuration directory
# If this property is not set, the realm name is '<http.authentication>-realm'
#http.digest.realm=local-realm

# Check the permissions of the realm file should be owner only
http.realm.file.permission.check=true

# LDAP group to Transformer role mapping
# the mapping is specified as the following pattern:
#    <ldap-group>:<transformer-role>(,<transformer-role>)*(;<ldap-group>:<transformer-role>(,<transformer-role>)*)*
# e.g.
#    Administrator:admin;Manager:manager;DevOP:creator;Tester:guest;
http.authentication.ldap.role.mapping=

# LDAP login module name as present in the JAAS config file.
# If no value is specified, the login module name is assumed to be "ldap"
ldap.login.module.name=ldap

# HTTP access control (CORS)
http.access.control.allow.origin=*
http.access.control.allow.headers=origin, content-type, cache-control, pragma, accept, authorization, x-requested-by, x-ss-user-auth-token, x-ss-rest-call
http.access.control.exposed.headers=X-SDC-LOG-PREVIOUS-OFFSET
http.access.control.allow.methods=GET, POST, PUT, DELETE, OPTIONS, HEAD

# Application callback inactivity timeout in milliseconds
# Max period of Transformer Spark application callback request inactivity, before stopping the pipeline with a run error.
# The default value is 30000 milliseconds (30 seconds)
transformer.driver.max.inactive.interval=30000

# A maximum number of times to retry an unsuccessful pipeline start on Databricks Cluster.
transformer.databricks.run.max.retries=2

# Minimal interval in milliseconds between the start of the failed run, and the subsequent retry run on the Databricks cluster.
transformer.databricks.run.retry.interval=10000

# Always resolve Kerberos properties
# This option allows Transformer pipelines to reference the keytab and principal declared in this properties file
# If you have a valid kerberos.client.principal and kerberos.client.keytab set below, and want to make them
# available to pipelines within this Transformer instance, then comment out the following line
#kerberos.client.alwaysResolveProperties=true

# The kerberos principal to use for the Kerberos session.
# It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',
# the hostname will be replaced with the actual complete hostname of Transformer as advertised by the
# unix command 'hostname -f'
kerberos.client.principal=transformer/_HOST@EXAMPLE.COM

# The location of the keytab file for the specified principal. If the path is relative, the keytab file will be
# looked under the Transformer configuration directory
kerberos.client.keytab=transformer.keytab

# Uncomment this line to disallow users to specify a keytab to use with pipelines.
# If this option is set to false, then the --proxy-user option will be set to spark-submit and the
# Transformer process will assume that a ticket cache exists already (i.e. is managed by a wrapper script
# such as k5start) and is therefore available for the spark-submit child process
#kerberos.pipeline.keytab.allowed=false

preview.maxBatchSize=1000
preview.maxBatches=10

# Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint
max.logtail.concurrent.requests=5

# Max number of concurrent WebSocket calls allowed
max.webSockets.concurrent.requests=15

# Pipeline Sharing / ACLs
pipeline.access.control.enabled=false

# Customize header title for TRANSFORMER UI
# You can pass any HTML tags here
# Example:
#   For Text  -  <span class="navbar-brand">New Brand Name</span>
#   For Image -  <img src="assets/add.png">
ui.header.title=<span class="navbar-brand">Transformer</span>

ui.local.help.base.url=/docs
ui.hosted.help.base.url=https://www.streamsets.com/documentation/transformer/3.7.0-SNAPSHOT/userguide/help
# ui.account.registration.url=

ui.refresh.interval.ms=2000
ui.jvmMetrics.refresh.interval.ms=4000

# If true TRANSFORMER UI will use WebSocket to fetch pipeline status/metrics/alerts otherwise UI will poll every few seconds
# to get the Pipeline status/metrics/alerts.
ui.enable.webSocket=true

# Number of changes supported by undo/redo functionality.
# UI archives Pipeline Configuration/Rules in browser memory to support undo/redo functionality.
ui.undo.limit=10

# SMTP configuration to send alert emails
# All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'
mail.transport.protocol=smtp
mail.smtp.host=localhost
mail.smtp.port=25
mail.smtp.auth=false
mail.smtp.starttls.enable=false
mail.smtps.host=localhost
mail.smtps.port=465
mail.smtps.auth=false
# If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,
# ${file("email-password.txt")} will load the value from the 'email-password.txt' file in the config directory (where this file is)
xmail.username=foo
xmail.password=${file("email-password.txt")}
# FROM email address to use for the messages
xmail.from.address=transformer@localhost

#Indicates the location where runtime configuration properties can be found.
#Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with
#'runtime.conf_'.
#A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration
#properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.
runtime.conf.location=embedded

# Java Security properties
#
# Any configuration prefixed with 'java.security.<property>' will be set on the static instance java.security.Security
# as part of TRANSFORMER bootstrap process. This will change JVM configuration and should not be used when embedding and running
# multiple TRANSFORMER instances inside the same JVM.
#
# We're explicitly overriding this to zero as JVM will default to -1 if security manager is active.
java.security.networkaddress.cache.ttl=0

# Stage specific configuration(s)
#
# The following config properties are for particular stages, please refer to their documentation for further details.
#
# Hadoop components
# Uncomment to enforce Hadoop components in TRANSFORMER to always impersonate current user rather then use the impersonation
# configuration option. Current user is a user who either started the pipeline or run preview.
#hadoop.always.impersonate.current.user=true
# Uncomment to enforce impersonated user name to be lower cased.
#hadoop.always.lowercase.user=true

# Indicate when Transformer runs on MapR cluster.
#hadoop.mapr.cluster=false

#Observer related

#The size of the queueName where the pipeline queues up data rule evaluation requests.
#Each request is for a stream and contains sampled records for all rules that apply to that lane.
observer.queue.size=100

#Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is
#once cache per data rule
observer.sampled.records.cache.size=100

#The time to wait before dropping a data rule evaluation request if the observer queueName is full.
observer.queue.offer.max.wait.time.ms=1000


#Maximum number of private classloaders to allow in Transformer.
#Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders
max.stage.private.classloaders=50

# Pipeline com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner pool
# Default value is sufficient to run 22 pipelines. One pipeline requires 5 Threads and pipelines share
# threads using thread pool. Approximate com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner thread pool size = (Number of Running Pipelines) * 2.2.
# Increasing this value will not increase parallelisation of individual pipelines.
com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.thread.pool.size=50

# Uncomment to disable starting all previously running pipelines on TRANSFORMER start up
#com.streamsets.datatransformer.dag.com.streamsets.datatransformer.dag.runner.boot.pipeline.restart=false

# Support bundles
#
# Uncomment if you need to disable the facility for automatic support bundle upload.
#bundle.upload.enabled=false
#
# Uncomment to automatically generate and upload bundle on various errors. Enable with caution, uploading bundle
# can be time consuming task (depending on size and internet speed) and pipelines can appear "frozen" during
# the upload especially when many pipelines are failing at the same time.
#bundle.upload.on_error=true

#
# Additional Configuration files to include in to the configuration.
# Value of this property is the name of the configuration file separated by commas.
#
config.includes=vault.properties,credential-stores.properties

#
# Pipeline State are cached for faster access.
# Specifies the maximum number of pipeline state entries the cache may contain.
store.pipeline.state.cache.maximum.size=100

# Specifies that each pipeline state entry should be automatically removed from the cache once a fixed duration
# has elapsed after the entry's creation, the most recent replacement of its value, or its last access.
# In minutes
store.pipeline.state.cache.expire.after.access=10

# uncomment to skip the Spark version >= 2.3.0 check performed in the Spark app launcher
#skipSparkVersionCheck=true

# base directory for temporary keytabs (used when keytabs are resolved from credential stores)
#transformer.temp-keytabs.location=/tmp/streamsets-transformer

#
# Copyright 2021 StreamSets Inc.
#

#
# Control Hub Enabled
# If true, HTTP Authentication for Transformer will be configured to use Control Hub SSO Authentication.
#
dpm.enabled=false

#
# Base URL of the Remote Service
# In a real deployment the security service must be encrypted (HTTPS)
#
dpm.base.url=http://localhost:18631

#
# Registration attempts.
# There is an exponential backoff that starts with 2 seconds and maxes out at 16 seconds.
#
dpm.registration.retry.attempts=5

#
# Frequency of validation of user and app authentication tokens.
# As part of this validation all information about the principal is refreshed.
#
dpm.security.validationTokenFrequency.secs=60

#
# Application Token
#
dpm.appAuthToken=@application-token.txt@

#
# Labels for this Transformer to report the Control Hub
#
dpm.remote.control.job.labels=all

#
# Transformer Ping Frequency to Control Hub (in milliseconds)
#
dpm.remote.control.ping.frequency=5000

#
# App to send remote control events
#
dpm.remote.control.events.recipient=jobrunner-app

#
# Apps to send Transformer process metrics (CPU Load and Heap Memory Usage)
#
dpm.remote.control.process.events.recipients=jobrunner-app,timeseries-app

#
# Frequency to send pipeline status events (all remote pipelines and local running pipelines) and
# Transformer process metrics like CPU load and heap memory usage
#
dpm.remote.control.status.events.interval = 60000


dpm.remote.deployment.id=

#
# Indicates if the redirection to Control Hub SSO is done using HTML META refresh.
# This is useful for environment that rewrite redirect headers.
#
http.meta.redirect.to.sso=false


# Uncomment to use 'user' as hadoop proxy user from the full user name 'user@org'. Below setting
# only takes effect when Transformer is control hub enabled and stage impersonation is set to true
#
#dpm.alias.name.enabled=true

#
# Component Type
#
dpm.componentType=transformer
